{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cornguo/colab/blob/main/GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXkngN0CQ7Ee"
      },
      "source": [
        "參考資料\n",
        "* https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html\n",
        "\n",
        "此筆記本為簡化後的版本"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQaxEanN75V"
      },
      "source": [
        "# 使用 GPT-2 生成金庸武俠小說\n",
        "\n",
        "- 發布時間：2019/09/07\n",
        "- 教學文章：<a href=\"https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html\" target=\"_blank\">直觀理解 GPT-2 語言模型並生成金庸武俠小說</a>\n",
        "- 本文作者：<a href=\"https://leemeng.tw/index.html\" target=\"_blank\">LeeMeng</a>\n",
        "\n",
        "\n",
        "<a href=\"https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html\" target=\"_blank\"><img src=\"https://leemeng.tw/images/gpt2/4_%E5%A4%A9%E9%BE%8D%E5%85%AB%E9%83%A8.jpg\" height=\"400\"></img></a>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzxT0o6oFi48"
      },
      "source": [
        "## 安裝函式庫"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdIQx81QEp1-",
        "cellView": "both"
      },
      "source": [
        "#@title　安裝 pytorch-transformers\n",
        "from IPython.display import clear_output\n",
        "!pip install pytorch-transformers\n",
        "!pip install tqdm\n",
        "!pip install torchsnooper\n",
        "!pip install transformers\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZxXngwoOVY9",
        "cellView": "both"
      },
      "source": [
        "#@title　基本 import\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import textwrap\n",
        "import torchsnooper\n",
        "import pytorch_transformers\n",
        "import torch.nn.functional as F\n",
        "from tqdm import trange\n",
        "from IPython.core.display import display, HTML"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFNjMSK3P9vj",
        "cellView": "form"
      },
      "source": [
        "#@title 下載 GPT-Chinese\n",
        "\n",
        "GITHUB_REPO = \"GPT2-Chinese\"\n",
        "!rm -rf {GITHUB_REPO}\n",
        "!git clone https://github.com/Morizeyao/{GITHUB_REPO}.git {GITHUB_REPO}\n",
        "if not GITHUB_REPO in sys.path:\n",
        "    sys.path += [GITHUB_REPO]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X64ITO30QAG8"
      },
      "source": [
        "## 預訓練金庸 GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hreyl8KFRNxx",
        "cellView": "form"
      },
      "source": [
        "#@title 下載、解壓縮模型\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Z8WdVYgBj01BHU4syjlY9qj3KBfEFP2D' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Z8WdVYgBj01BHU4syjlY9qj3KBfEFP2D\" -O 10layers_12heads_1024len_768embd_full_corpus_16bsize.zip && rm -rf /tmp/cookies.txt\n",
        "\n",
        "pretrained_model = '10layers_12heads_1024len_768embd_full_corpus_16bsize'\n",
        "\n",
        "!unzip {pretrained_model}.zip\n",
        "sagemaker_base_path = 'home/ec2-user/SageMaker/tmp/GPT2-Chinese'\n",
        "config_file = 'config.json'\n",
        "model_ckpt = \"pytorch_model.bin\"\n",
        "vocab_file = \"vocab_small.txt\"\n",
        "\n",
        "!rm {config_file} {model_ckpt} {vocab_file}\n",
        "\n",
        "!mv {sagemaker_base_path}/model/{pretrained_model}/{config_file} {config_file}\n",
        "!mv {sagemaker_base_path}/model/{pretrained_model}/{model_ckpt} {model_ckpt}\n",
        "!mv {sagemaker_base_path}/cache/{vocab_file} {vocab_file}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usQd6bTcSDcV",
        "cellView": "both"
      },
      "source": [
        "#@title 載入模型以及 Tokenizer\n",
        "\n",
        "from tokenizations import tokenization_bert\n",
        "\n",
        "# make model output attentions\n",
        "config = pytorch_transformers.GPT2Config.from_json_file(config_file)\n",
        "config.output_attentions = True\n",
        "\n",
        "\n",
        "model = pytorch_transformers.GPT2LMHeadModel.from_pretrained(\".\", config=config)\n",
        "tokenizer = tokenization_bert.BertTokenizer(vocab_file=vocab_file)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "clear_output()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyGHa7qUTcOG",
        "cellView": "both"
      },
      "source": [
        "#@title 生成用的 utility functions\n",
        "\n",
        "def is_word(word):\n",
        "    for item in list(word):\n",
        "        if item not in 'qwertyuiopasdfghjklzxcvbnm':\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "def fast_sample_sequence(model,context,length,temperature=1, top_k=0, top_p=0.0,device='cpu'):\n",
        "    inputs = torch.LongTensor(context).view(1, -1).to(device)\n",
        "    if len(context) > 1:\n",
        "        _, past = model(inputs[:, :-1], None)[:2]\n",
        "        prev = inputs[:, -1].view(1, -1)\n",
        "    else:\n",
        "        past = None\n",
        "        prev = inputs\n",
        "    generate = [] + context\n",
        "    with torch.no_grad():\n",
        "        for i in trange(length):\n",
        "            output = model(prev, past=past)\n",
        "            output, past = output[:2]\n",
        "            output = output[-1].squeeze(0) / temperature\n",
        "            filtered_logits = top_k_top_p_filtering(output, top_k=top_k, top_p=top_p)\n",
        "            next_token = torch.multinomial(torch.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            \n",
        "            # redraw if [UNK]\n",
        "            if next_token.unsqueeze(0) != 100:\n",
        "                generate.append(next_token.item())\n",
        "                prev = next_token.view(1, 1)\n",
        "\n",
        "    return generate\n",
        "\n",
        "\n",
        "def get_html(context, generated_text, novel_name='', algorithm=''):\n",
        "    if generated_text[-1] != '。':\n",
        "        if generated_text[-1] == '，':\n",
        "            generated_text= generated_text[:-1]\n",
        "        generated_text += ' ...'\n",
        "    \n",
        "    html = \"\"\"\n",
        "    <html>\n",
        "    <head>\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "    <style>\n",
        "    * {\n",
        "      box-sizing: border-box;\n",
        "    }\n",
        "\n",
        "    /* Create two unequal columns that floats next to each other */\n",
        "    .column {\n",
        "      float: left;\n",
        "      padding: 10px;\n",
        "    }\n",
        "\n",
        "    .left {\n",
        "      width: 12%;\n",
        "    }\n",
        "\n",
        "    .right {\n",
        "      width: 35%;\n",
        "    }\n",
        "\n",
        "    /* Clear floats after the columns */\n",
        "    .row:after {\n",
        "      content: \"\";\n",
        "      display: table;\n",
        "      clear: both;\n",
        "    }\n",
        "    </style>\n",
        "    </head>\n",
        "    <body>\n",
        "\n",
        "    <div style=\"background-color:#404452 !important\">\n",
        "        <div class=\"row\">\n",
        "          <div class=\"column left\">\n",
        "            <h2 style=\"color: #bfbaba;text-align:center\">\n",
        "                novel_name\n",
        "                前文脈絡\n",
        "            </h2>\n",
        "          </div>\n",
        "          <div class=\"column right\">\n",
        "            <h3 style=\"color: white;line-height: 1.5\">context</h3>\n",
        "          </div>\n",
        "        </div>\n",
        "\n",
        "        <hr/>\n",
        "\n",
        "        <div class=\"row\">\n",
        "          <div class=\"column left\">\n",
        "            <h2 style=\"color: #bfbaba;text-align:center\">\n",
        "                algorithm\n",
        "                生成結果\n",
        "            </h2>\n",
        "          </div>\n",
        "          <div class=\"column right\">\n",
        "            <h3 style=\"color: white;line-height: 1.5\">generated_text</h3>\n",
        "\n",
        "          </div>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    </body>\n",
        "    </html>\n",
        "\n",
        "\n",
        "    \"\"\".replace('context', context).replace('generated_text', generated_text).replace(\"novel_name\", f'《{novel_name}》<br/>')\n",
        "    \n",
        "    if not algorithm:\n",
        "        html = html.replace(\"algorithm\", \"\")\n",
        "    else:\n",
        "        html = html.replace(\"algorithm\", f'{algorithm}<br/>')\n",
        "    \n",
        "    \n",
        "    return html\n",
        "\n",
        "\n",
        "def generate(context, topk, topp, temperature, device, line_len=40, novel_name=''):\n",
        "    context_tokens = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(context))\n",
        "        \n",
        "    # auto-regressive\n",
        "    out = fast_sample_sequence(\n",
        "        model=model, length=length,\n",
        "        context=context_tokens,\n",
        "        temperature=temperature, top_k=topk, top_p=topp, device=device\n",
        "    )\n",
        "\n",
        "    # rendering\n",
        "    tokens = tokenizer.convert_ids_to_tokens(out)\n",
        "\n",
        "    for i, item in enumerate(tokens):\n",
        "        if item == '[MASK]':\n",
        "            tokens[i] = ''\n",
        "        if item == '[CLS]' or item == '[SEP]':\n",
        "            tokens[i] = '\\n'\n",
        "    \n",
        "    generated_text = ''.join(tokens).strip().replace(context, '')\n",
        "    html = get_html(context, generated_text, novel_name=novel_name)\n",
        "    \n",
        "    return html, generated_text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgexLl0tTm2_"
      },
      "source": [
        "## 生成新的金庸橋段\n",
        "如果你有任何有趣的新橋段想分享，可以截圖到<a href=\"https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html\" target=\"_blank\">直觀理解 GPT-2 語言模型並生成金庸武俠小說</a>，或在 <a href=\"https://www.facebook.com/LeeMengTaiwan\" target=\"_blank\">Facebook 與我分享</a>。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAy8GlawVAq4"
      },
      "source": [
        "#@ title 生成參數、Decoding 策略設定（生成溫度等）\n",
        "nsamples = 1\n",
        "batch_size = 1\n",
        "length = model.config.n_ctx // 2\n",
        "\n",
        "topk = 30\n",
        "topp = 0\n",
        "temperature = 0.5"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEVwRfoOTv7h"
      },
      "source": [
        "#@title 前文脈絡範例\n",
        "\n",
        "# 改變此變數來選擇不同的前文脈絡。當然你也可以自行填加\n",
        "sample_idx = 0\n",
        "\n",
        "\n",
        "# 飛雪連天射白鹿，笑書神俠倚碧鴛。\n",
        "sampled_contexts =[\n",
        "    ('飛狐外傳', '胡斐行動快極，右手彎處，抱住了程靈素的纖腰，倒縱出門，經過房門時飛起一腿，踢在門板之上。'),\n",
        "    ('雪山飛狐', '胡一刀抱著孩子走進房去，那房間的板壁極薄，只聽夫人問道：‘大哥，是誰來了啊？’'),\n",
        "    ('連城訣', '戚芳躲在狄雲背後，也不見禮。只點頭笑了笑。'),\n",
        "    ('天龍八部', '段譽和王語嫣面面相對，呼吸可聞，雖身處污泥，心中卻充滿了喜樂之情，誰也沒想到要爬出井去。兩人同時慢慢的伸手出來，四手相握，心意相通。'),\n",
        "    ('射雕英雄傳', '黃蓉眼圈兒一紅，道：「爹爹不要我啦。」郭靖道：「乾麼呀？」'),\n",
        "    ('白馬嘯西風', '史仲俊和白馬李三的妻子上官虹原是同門師兄妹，兩人自幼一起學藝。'),\n",
        "    ('鹿鼎記', '韋小寶只覺滿鼻子都是濃香，懷中抱著的那女子全身光溜溜地，竟然一絲不掛，又覺那女子反手過來，抱住了自己，心中一陣迷迷糊糊，聽得雙兒低聲問道：「相公，怎麼了？」韋小寶唔唔幾聲，待要答話，懷中那女子伸嘴吻住了他嘴巴，登時說不出話來。'),\n",
        "    ('笑傲江湖', '令狐沖淡淡一笑，道：「原要多謝兩位的救命之恩。」王家駒聽他語氣，知他說的乃是反話，更加有氣，大聲道：「你是華山派掌門大弟子，連洛陽城中幾個流氓混混也對付不了，嘿嘿，旁人不知，豈不是要說你浪得虛名？」'),\n",
        "    ('書劍恩仇錄', '陳家洛在下首站定，微一拱手，說道：「請賜招。」'),\n",
        "    ('神鵰俠侶', '黃蓉見楊過中毒極深，低聲道：「咱們先投客店，到城裡配幾味藥。」'),\n",
        "    ('俠客行', '石破天見茶几上放著兩碗清茶，便自己左手取了一碗，右手將另一碗遞過去。陳衝之既怕茶中有毒，又怕石破天乘機出手，不敢伸手去接，反退了一步，嗆啷一聲，一隻瓷碗在地下摔得粉碎。'),\n",
        "    ('倚天屠龍記', '張無忌見三名老僧在片刻間連斃崑崙派四位高手，舉重若輕，游刃有餘，武功之高，實是生平罕見，比之鹿杖客和鶴筆翁似乎猶有過之，縱不如太師父張三丰之深不可測，卻也到了神而明之的境界。'),\n",
        "    ('碧血劍', '張朝唐聽到這裡，才知道這神像原來是連破清兵、擊斃清太祖努爾哈赤、使清人聞名喪膽的薊遼督師袁崇煥。'),\n",
        "    ('鴛鴦刀', '蕭中慧一聽父親說起這對寶刀，當即躍躍欲試。'),\n",
        "    ('天龍八部', '蕭峯喝道：「你就想走？天下有這等便宜事？你父親身上有病，大丈夫不屑乘人之危，且放了他過去。你可沒病沒痛！」慕容復氣往上衝，喝道：「那我便接蕭兄的高招。」蕭峯更不打話，呼的一掌，一招降龍十八掌中的「見龍在田」，向慕容復猛擊過去。他見藏經閣中地勢狹隘，高手群集，不便久鬥，是以使上了十成力，要在數掌之間便取了敵人性命。慕容復見他掌勢兇惡，當即運起平生之力，要以「斗轉星移」之術化解。'),\n",
        "]\n",
        "\n",
        "sample = sampled_contexts[sample_idx]\n",
        "novel_name, context = sample"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKMwtCFaUzm1"
      },
      "source": [
        "html, generated_text = generate(context, topk, topp, temperature, device, novel_name=novel_name)\n",
        "display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}