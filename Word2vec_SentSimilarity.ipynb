{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2vec-SentSimilarity",
      "provenance": [],
      "authorship_tag": "ABX9TyOQgQPG300I8re9uzCsHoi/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cornguo/colab/blob/main/Word2vec_SentSimilarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foU7bGquFaSm"
      },
      "source": [
        "國教院分詞器，改寫為 Python3 相容語法\n",
        "\n",
        "參考資料\n",
        "* https://github.com/naernlp/Segmentor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz4CLcTDFlf_"
      },
      "source": [
        "安裝 CRF++"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1_8eDcUEMYf"
      },
      "source": [
        "!curl -L \"https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7QVR6VXJ5dWExSTQ\" | tar xzvf -\n",
        "!cd CRF++-0.58; ./configure; make; sudo make install; cd python; sudo python setup.py install\n",
        "!ln -s /usr/local/lib/libcrfpp.so.0 /usr/lib/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yMjICjYFoWU"
      },
      "source": [
        "安裝國教院分詞系統"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRCEQUDqEV4p"
      },
      "source": [
        "!git clone --branch python3 https://github.com/cornguo/Segmentor\n",
        "!curl https://120.127.233.228/download/Segmentor/naer-segmentor-models-20160318.tar.gz -k | tar xzvf - -C Segmentor/Segmentor\n",
        "!cd Segmentor; sudo python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR4YKnXj-fh7"
      },
      "source": [
        "!pip3 install gensim\n",
        "!pip3 install h5py\n",
        "!pip3 install gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgSQs8S3-ljp"
      },
      "source": [
        "!wget https://cornguo.storage.googleapis.com/wiki2019tw_word2vec_cbow_d200.zip\n",
        "!unzip wiki2019tw_word2vec_cbow_d200.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKw2t3bXFtkf"
      },
      "source": [
        "測試"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8XE_pwhEwjv"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "from Segmentor import *\n",
        "from gensim.models import word2vec\n",
        "from gensim import models\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "\n",
        "# 載入斷詞器\n",
        "segmenter = Segmentor()\n",
        "# 載入 Word2vec model\n",
        "model = models.Word2Vec.load('./wiki2019tw_word2vec_cbow_d200/wiki2019tw_word2vec_cbow_d200.model')\n",
        "\n",
        "def getSentVect(query):\n",
        "  # 進行斷詞\n",
        "  words = segmenter.segment(query)\n",
        "  wordCnt = 0\n",
        "  # 計算句子的語義向量 (向量平均)\n",
        "  sentVect = np.zeros((200, ), dtype='float32')\n",
        "  for word in words:\n",
        "    #print(\"processing:\", word)\n",
        "    if word in model.wv.index2word:\n",
        "      #print(\"found\")\n",
        "      wordCnt += 1\n",
        "      sentVect = np.add(sentVect, model.wv[word])\n",
        "\n",
        "  if wordCnt > 0:\n",
        "    np.divide(sentVect, wordCnt)\n",
        "\n",
        "  #print(\"sentence word cnt:\", wordCnt, \"avg vector:\", sentVect)\n",
        "  return sentVect\n",
        "\n",
        "def getSim(query1, query2):\n",
        "  sentVect1 = getSentVect(query1)\n",
        "  sentVect2 = getSentVect(query2)\n",
        "  sim = 1 - spatial.distance.cosine(sentVect1, sentVect2)\n",
        "  return sim\n",
        "\n",
        "while True:\n",
        "  # 取得輸入文字\n",
        "  print(\"input 2 sentences, seperated by a space:\")\n",
        "  query = input()\n",
        "\n",
        "  queries = query.split()\n",
        "  # 取得句子的語義向量\n",
        "  sim = getSim(queries[0], queries[1])\n",
        "  print(\"similarity of\", queries[0], \"and\", queries[1], \"is\", sim)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}