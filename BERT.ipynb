{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cornguo/colab/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIJ1r8cSw2Up"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DpsmqwHXijM"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7UrSdZDXtgN"
      },
      "source": [
        "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
        "\n",
        "# 取得此預訓練模型所使用的 tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp2SgBFTppDj"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/cornguo/colab/main/news.zip\n",
        "!unzip news.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai4aifAlqaZZ"
      },
      "source": [
        "\"\"\"\n",
        "前處理原始的訓練數據集。\n",
        "你不需了解細節，只需要看註解了解邏輯或是輸出的數據格式即可\n",
        "\"\"\"\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# 簡單的數據清理，去除空白標題的 examples\n",
        "df_train = pd.read_csv(\"/content/news/train.csv\")\n",
        "df_train = df_train[['text', 'keyword']]\n",
        "\n",
        "# 剔除過長的樣本以避免 BERT 無法將整個輸入序列放入記憶體不多的 GPU\n",
        "MAX_LENGTH = 250\n",
        "df_train['text'] = df_train['text'].apply(lambda x: x[:MAX_LENGTH])\n",
        "\n",
        "# 去除不必要的欄位並重新命名兩標題的欄位名\n",
        "df_train = df_train.reset_index()\n",
        "df_train = df_train.loc[:, ['text', 'keyword']]\n",
        "\n",
        "# idempotence, 將處理結果另存成 tsv 供 PyTorch 使用\n",
        "df_train.to_csv(\"train_news.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(\"訓練樣本數：\", len(df_train))\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sS4ImYL7JzR"
      },
      "source": [
        "df_train.keyword.value_counts() / len(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfd7o8GFq7MA"
      },
      "source": [
        "df_test = pd.read_csv(\"/content/news/valid.csv\")\n",
        "df_test = df_test.loc[:, [\"text\",'id']]\n",
        "\n",
        "# 剔除過長的樣本以避免 BERT 無法將整個輸入序列放入記憶體不多的 GPU\n",
        "MAX_LENGTH = 250\n",
        "df_test['text'] = df_test['text'].apply(lambda x: x[:MAX_LENGTH])\n",
        "\n",
        "df_test.to_csv(\"test_news.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(\"預測樣本數：\", len(df_test))\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsYeFc_er6LL"
      },
      "source": [
        "len(df_test.iloc[3,].text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA8EHXb7tQwt"
      },
      "source": [
        "\"\"\"\n",
        "實作一個可以用來讀取訓練 / 測試集的 Dataset，這是你需要徹底了解的部分。\n",
        "此 Dataset 每次將 tsv 裡的一筆成對句子轉換成 BERT 相容的格式，並回傳 3 個 tensors：\n",
        "- tokens_tensor：兩個句子合併後的索引序列，包含 [CLS] 與 [SEP]\n",
        "- segments_tensor：可以用來識別兩個句子界限的 binary tensor\n",
        "- label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
        "\"\"\"\n",
        "from torch.utils.data import Dataset\n",
        " \n",
        "    \n",
        "class NewsDataset(Dataset):\n",
        "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
        "    def __init__(self, mode, tokenizer):\n",
        "        assert mode in [\"train_news\", \"test_news\"]  # 一般訓練你會需要 dev set\n",
        "        self.mode = mode\n",
        "        # 大數據你會需要用 iterator=True\n",
        "        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
        "        self.len = len(self.df)\n",
        "        self.label_map = {'疫情': 0, '投資': 1, '三倍券': 2, '政治': 3, '天氣': 4, '大自然': 5, '社會新聞': 6, '生活': 7, '名人': 8, '車': 9}\n",
        "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
        "    \n",
        "    # 定義回傳一筆訓練 / 測試數據的函式\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == \"test_news\":\n",
        "            #text= self.df.iloc[idx, 0].values\n",
        "            text= self.df.iloc[idx, 0]\n",
        "            label_tensor = None\n",
        "        else:\n",
        "            text, label = self.df.iloc[idx, :].values\n",
        "            # 將 label 文字也轉換成索引方便轉換成 tensor\n",
        "            label_id = self.label_map[label]\n",
        "            label_tensor = torch.tensor(label_id)\n",
        "            \n",
        "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
        "        word_pieces = [\"[CLS]\"]\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        word_pieces += tokens + [\"[SEP]\"]\n",
        "        len_a = len(word_pieces)\n",
        "        \n",
        "        # 將整個 token 序列轉換成索引序列\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
        "        tokens_tensor = torch.tensor(ids)\n",
        "        \n",
        "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
        "        segments_tensor = torch.tensor([0] * len_a, \n",
        "                                        dtype=torch.long)\n",
        "        \n",
        "        return (tokens_tensor, segments_tensor, label_tensor)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    \n",
        "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
        "trainset = NewsDataset(\"train_news\", tokenizer=tokenizer)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HHI8j28tSJe"
      },
      "source": [
        "# 選擇第一個樣本\n",
        "sample_idx = 5\n",
        "\n",
        "# 將原始文本拿出做比較\n",
        "text, label = trainset.df.iloc[sample_idx].values\n",
        "\n",
        "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
        "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
        "\n",
        "# 將 tokens_tensor 還原成文本\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
        "combined_text = \"\".join(tokens)\n",
        "\n",
        "# 渲染前後差異，毫無反應就是個 print。可以直接看輸出結果\n",
        "print(f\"\"\"[原始文本]\n",
        "句子：{text}\n",
        "分類：{label}\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mAeRMGKtULp"
      },
      "source": [
        "\"\"\"\n",
        "實作可以一次回傳一個 mini-batch 的 DataLoader\n",
        "這個 DataLoader 吃我們上面定義的 `NewsDataset`，\n",
        "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
        "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
        "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
        "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
        "- label_ids       : (batch_size)\n",
        "\"\"\"\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# 這個函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是\n",
        "# 剛剛定義的 `NewsDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
        "# - tokens_tensor\n",
        "# - segments_tensor\n",
        "# - label_tensor\n",
        "# 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
        "def create_mini_batch(samples):\n",
        "    tokens_tensors = [s[0] for s in samples]\n",
        "    segments_tensors = [s[1] for s in samples]\n",
        "    \n",
        "    # 測試集有 labels\n",
        "    if samples[0][2] is not None:\n",
        "        label_ids = torch.stack([s[2] for s in samples])\n",
        "    else:\n",
        "        label_ids = None\n",
        "    \n",
        "    # zero pad 到同一序列長度\n",
        "    tokens_tensors = pad_sequence(tokens_tensors, \n",
        "                                  batch_first=True)\n",
        "    segments_tensors = pad_sequence(segments_tensors, \n",
        "                                    batch_first=True)\n",
        "    \n",
        "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
        "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
        "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
        "                                dtype=torch.long)\n",
        "    masks_tensors = masks_tensors.masked_fill(\n",
        "        tokens_tensors != 0, 1)\n",
        "    \n",
        "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
        "\n",
        "\n",
        "# 初始化一個每次回傳 64 個訓練樣本的 DataLoader\n",
        "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
        "BATCH_SIZE = 8\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
        "                         collate_fn=create_mini_batch)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axp1Sn0EtV7B"
      },
      "source": [
        "data = next(iter(trainloader))\n",
        "\n",
        "tokens_tensors, segments_tensors, \\\n",
        "    masks_tensors, label_ids = data\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMs0UPxotXXe"
      },
      "source": [
        "# 載入一個可以做中文多分類任務的模型，n_class = 10\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "NUM_LABELS = 10\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# high-level 顯示此模型裡的 modules\n",
        "print(\"\"\"\n",
        "name            module\n",
        "----------------------\"\"\")\n",
        "for name, module in model.named_children():\n",
        "    if name == \"bert\":\n",
        "        for n, _ in module.named_children():\n",
        "            print(f\"{name}:{n}\")\n",
        "    else:\n",
        "        print(\"{:15} {}\".format(name, module))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1N2iZAXtY_i"
      },
      "source": [
        "\"\"\"\n",
        "定義一個可以針對特定 DataLoader 取得模型預測結果以及分類準確度的函式\n",
        "之後也可以用來生成上傳到 Kaggle 競賽的預測結果\n",
        "\n",
        "2019/11/22 更新：在將 `tokens`、`segments_tensors` 等 tensors\n",
        "丟入模型時，強力建議指定每個 tensor 對應的參數名稱，以避免 HuggingFace\n",
        "更新 repo 程式碼並改變參數順序時影響到我們的結果。\n",
        "\"\"\"\n",
        "\n",
        "def get_predictions(model, dataloader, compute_acc=False):\n",
        "    predictions = None\n",
        "    correct = 0\n",
        "    total = 0\n",
        "      \n",
        "    with torch.no_grad():\n",
        "        # 遍巡整個資料集\n",
        "        for data in dataloader:\n",
        "            # 將所有 tensors 移到 GPU 上\n",
        "            if next(model.parameters()).is_cuda:\n",
        "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
        "            \n",
        "            \n",
        "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
        "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
        "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
        "            outputs = model(input_ids=tokens_tensors, \n",
        "                            token_type_ids=segments_tensors, \n",
        "                            attention_mask=masks_tensors)\n",
        "            \n",
        "            logits = outputs[0]\n",
        "            _, pred = torch.max(logits.data, 1)\n",
        "            \n",
        "            # 用來計算訓練集的分類準確率\n",
        "            if compute_acc:\n",
        "                labels = data[3]\n",
        "                total += labels.size(0)\n",
        "                correct += (pred == labels).sum().item()\n",
        "                \n",
        "            # 將當前 batch 記錄下來\n",
        "            if predictions is None:\n",
        "                predictions = pred\n",
        "            else:\n",
        "                predictions = torch.cat((predictions, pred))\n",
        "    \n",
        "    if compute_acc:\n",
        "        acc = correct / total\n",
        "        return predictions, acc\n",
        "    return predictions\n",
        "    \n",
        "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "model = model.to(device)\n",
        "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
        "print(\"classification acc:\", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTwq3W_ntadg"
      },
      "source": [
        "def get_learnable_params(module):\n",
        "    return [p for p in module.parameters() if p.requires_grad]\n",
        "     \n",
        "model_params = get_learnable_params(model)\n",
        "clf_params = get_learnable_params(model.classifier)\n",
        "\n",
        "print(f\"\"\"\n",
        "整個分類模型的參數量：{sum(p.numel() for p in model_params)}\n",
        "線性分類器的參數量：{sum(p.numel() for p in clf_params)}\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOfPeGDuwWr9"
      },
      "source": [
        "%%time\n",
        "\n",
        "# 訓練模式\n",
        "model.train()\n",
        "\n",
        "# 使用 Adam Optim 更新整個分類模型的參數\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "\n",
        "EPOCHS = 5  # 幸運數字\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    for data in trainloader:\n",
        "        \n",
        "        tokens_tensors, segments_tensors, \\\n",
        "        masks_tensors, labels = [t.to(device) for t in data]\n",
        "\n",
        "        # 將參數梯度歸零\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass\n",
        "        outputs = model(input_ids=tokens_tensors, \n",
        "                        token_type_ids=segments_tensors, \n",
        "                        attention_mask=masks_tensors, \n",
        "                        labels=labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        # backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        # 紀錄當前 batch loss\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    # 計算分類準確率\n",
        "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
        "\n",
        "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
        "          (epoch + 1, running_loss, acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZg2OM8dwYFZ"
      },
      "source": [
        "%%time\n",
        "# 建立測試集。這邊我們可以用跟訓練時不同的 batch_size，看你 GPU 多大\n",
        "testset = NewsDataset(\"test_news\", tokenizer=tokenizer)\n",
        "testloader = DataLoader(testset, batch_size=8, \n",
        "                        collate_fn=create_mini_batch)\n",
        "\n",
        "# 用分類模型預測測試集\n",
        "predictions = get_predictions(model, testloader)\n",
        "\n",
        "# 用來將預測的 label id 轉回 label 文字\n",
        "index_map = {v: k for k, v in testset.label_map.items()}\n",
        "\n",
        "# 生成 Kaggle 繳交檔案\n",
        "df = pd.DataFrame({\"Category\": predictions.tolist()})\n",
        "df['Category'] = df.Category.apply(lambda x: index_map[x])\n",
        "df_pred = pd.concat([testset.df.loc[:, [\"id\"]], \n",
        "                          df.loc[:, 'Category']], axis=1)\n",
        "df_pred.to_csv('bert_1_prec_training_samples.csv', index=False)\n",
        "df_pred.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMRQTyPzwaoH"
      },
      "source": [
        "valid = pd.read_csv(\"/content/news/valid.csv\")\n",
        "df_pred.merge(valid, on=['id'])\n",
        "df_pred.to_csv('pred.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "!cat pred.csv"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}